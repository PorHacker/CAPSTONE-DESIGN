{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:32:36.651545500Z",
     "start_time": "2023-11-13T18:32:36.616579300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:42.128778100Z",
     "start_time": "2023-11-13T18:32:36.629999900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(136265, 30, 100)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "actions = [\n",
    "    '나',\n",
    "    '개학',\n",
    "    '홍익대학교',\n",
    "    '가다',\n",
    "    '시험',\n",
    "    '시작'\n",
    "]\n",
    "\n",
    "# Directory containing your .npy files\n",
    "directory = 'dataset/seq/'\n",
    "\n",
    "# Initialize an empty list to store the loaded data\n",
    "data_list = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        loaded_data = np.load(file_path)\n",
    "        data_list.append(loaded_data)\n",
    "\n",
    "# Concatenate all loaded data into a single array\n",
    "data = np.concatenate(data_list, axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:42.952210900Z",
     "start_time": "2023-11-13T18:33:42.117610400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136265, 30, 99)\n",
      "(136265,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:42.953214200Z",
     "start_time": "2023-11-13T18:33:42.285924700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(136265, 6)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:48.924637900Z",
     "start_time": "2023-11-13T18:33:42.595905700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122638, 30, 99) (122638, 6)\n",
      "(13627, 30, 99) (13627, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:49.083327Z",
     "start_time": "2023-11-13T18:33:48.933147100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(30, 99)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T18:33:49.937664700Z",
     "start_time": "2023-11-13T18:33:48.964199300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 30, 64)            41984     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,654\n",
      "Trainable params: 55,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=x_train.shape[1:3]),\n",
    "    LSTM(32, activation='relu', return_sequences=False, input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-11-13T20:29:28.535340Z",
     "start_time": "2023-11-13T18:33:49.916919900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 1.6822 - acc: 0.5558\n",
      "Epoch 1: val_acc improved from -inf to 0.63095, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 98s 24ms/step - loss: 1.6822 - acc: 0.5558 - val_loss: 0.9829 - val_acc: 0.6310 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 1.0011 - acc: 0.6395\n",
      "Epoch 2: val_acc improved from 0.63095 to 0.71512, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 96s 25ms/step - loss: 1.0011 - acc: 0.6395 - val_loss: 0.8059 - val_acc: 0.7151 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.7479\n",
      "Epoch 3: val_acc improved from 0.71512 to 0.78851, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 89s 23ms/step - loss: 0.6872 - acc: 0.7479 - val_loss: 0.5872 - val_acc: 0.7885 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.9319 - acc: 0.6628\n",
      "Epoch 4: val_acc did not improve from 0.78851\n",
      "3833/3833 [==============================] - 89s 23ms/step - loss: 0.9318 - acc: 0.6628 - val_loss: 0.7372 - val_acc: 0.7255 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.8087 - acc: 0.7031\n",
      "Epoch 5: val_acc did not improve from 0.78851\n",
      "3833/3833 [==============================] - 92s 24ms/step - loss: 0.8087 - acc: 0.7031 - val_loss: 0.7006 - val_acc: 0.7373 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6159 - acc: 0.7698\n",
      "Epoch 6: val_acc improved from 0.78851 to 0.79812, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 99s 26ms/step - loss: 0.6159 - acc: 0.7698 - val_loss: 0.5490 - val_acc: 0.7981 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.5400 - acc: 0.8011\n",
      "Epoch 7: val_acc did not improve from 0.79812\n",
      "3833/3833 [==============================] - 91s 24ms/step - loss: 0.5400 - acc: 0.8011 - val_loss: 0.5803 - val_acc: 0.7881 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 2.9727 - acc: 0.7855\n",
      "Epoch 8: val_acc did not improve from 0.79812\n",
      "3833/3833 [==============================] - 90s 24ms/step - loss: 2.9719 - acc: 0.7854 - val_loss: 0.8106 - val_acc: 0.7173 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.7553\n",
      "Epoch 9: val_acc did not improve from 0.79812\n",
      "3833/3833 [==============================] - 91s 24ms/step - loss: 0.6460 - acc: 0.7553 - val_loss: 0.5446 - val_acc: 0.7918 - lr: 0.0010\n",
      "Epoch 10/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6895 - acc: 0.7472\n",
      "Epoch 10: val_acc improved from 0.79812 to 0.79952, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 96s 25ms/step - loss: 0.6896 - acc: 0.7472 - val_loss: 0.5674 - val_acc: 0.7995 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.7921\n",
      "Epoch 11: val_acc improved from 0.79952 to 0.81280, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 95s 25ms/step - loss: 0.5770 - acc: 0.7921 - val_loss: 0.5166 - val_acc: 0.8128 - lr: 0.0010\n",
      "Epoch 12/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8289\n",
      "Epoch 12: val_acc improved from 0.81280 to 0.84120, saving model to models\\model_r8.h5\n",
      "3833/3833 [==============================] - 94s 25ms/step - loss: 0.4658 - acc: 0.8289 - val_loss: 0.4303 - val_acc: 0.8412 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 75.2621 - acc: 0.8185\n",
      "Epoch 13: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 92s 24ms/step - loss: 75.3171 - acc: 0.8185 - val_loss: 851.7610 - val_acc: 0.1843 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 150.0333 - acc: 0.3452\n",
      "Epoch 14: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 95s 25ms/step - loss: 150.0167 - acc: 0.3452 - val_loss: 7.5556 - val_acc: 0.3573 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 2.6067 - acc: 0.5138\n",
      "Epoch 15: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 90s 23ms/step - loss: 2.6066 - acc: 0.5138 - val_loss: 1.9133 - val_acc: 0.5690 - lr: 0.0010\n",
      "Epoch 16/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 2.3821 - acc: 0.5254\n",
      "Epoch 16: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 85s 22ms/step - loss: 2.3821 - acc: 0.5254 - val_loss: 1.6255 - val_acc: 0.5256 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 1.2988 - acc: 0.5781\n",
      "Epoch 17: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 84s 22ms/step - loss: 1.2986 - acc: 0.5781 - val_loss: 0.9181 - val_acc: 0.6560 - lr: 0.0010\n",
      "Epoch 18/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.6657\n",
      "Epoch 18: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 85s 22ms/step - loss: 0.8781 - acc: 0.6657 - val_loss: 0.8088 - val_acc: 0.6869 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.6906\n",
      "Epoch 19: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 87s 23ms/step - loss: 0.7956 - acc: 0.6907 - val_loss: 0.7477 - val_acc: 0.7053 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7021\n",
      "Epoch 20: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 85s 22ms/step - loss: 0.8413 - acc: 0.7021 - val_loss: 0.7323 - val_acc: 0.7112 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.7042 - acc: 0.7244\n",
      "Epoch 21: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 85s 22ms/step - loss: 0.7042 - acc: 0.7244 - val_loss: 0.6709 - val_acc: 0.7361 - lr: 0.0010\n",
      "Epoch 22/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6599 - acc: 0.7400\n",
      "Epoch 22: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3833/3833 [==============================] - 86s 22ms/step - loss: 0.6599 - acc: 0.7400 - val_loss: 0.6773 - val_acc: 0.7320 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6334 - acc: 0.7569\n",
      "Epoch 23: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 88s 23ms/step - loss: 0.6334 - acc: 0.7569 - val_loss: 0.5703 - val_acc: 0.7773 - lr: 5.0000e-04\n",
      "Epoch 24/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.7004 - acc: 0.7428\n",
      "Epoch 24: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 86s 22ms/step - loss: 0.7004 - acc: 0.7428 - val_loss: 0.6215 - val_acc: 0.7573 - lr: 5.0000e-04\n",
      "Epoch 25/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.7670\n",
      "Epoch 25: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 86s 23ms/step - loss: 0.5932 - acc: 0.7670 - val_loss: 0.5719 - val_acc: 0.7773 - lr: 5.0000e-04\n",
      "Epoch 26/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.7884\n",
      "Epoch 26: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 87s 23ms/step - loss: 0.5449 - acc: 0.7884 - val_loss: 0.5333 - val_acc: 0.7961 - lr: 5.0000e-04\n",
      "Epoch 27/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 5.8482 - acc: 0.6932\n",
      "Epoch 27: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 85s 22ms/step - loss: 5.8482 - acc: 0.6932 - val_loss: 0.8499 - val_acc: 0.6998 - lr: 5.0000e-04\n",
      "Epoch 28/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.9834 - acc: 0.6781\n",
      "Epoch 28: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 86s 22ms/step - loss: 0.9833 - acc: 0.6781 - val_loss: 0.7860 - val_acc: 0.6968 - lr: 5.0000e-04\n",
      "Epoch 29/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.9206 - acc: 0.6689\n",
      "Epoch 29: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 88s 23ms/step - loss: 0.9205 - acc: 0.6689 - val_loss: 0.8066 - val_acc: 0.6956 - lr: 5.0000e-04\n",
      "Epoch 30/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.8205 - acc: 0.6931\n",
      "Epoch 30: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 89s 23ms/step - loss: 0.8205 - acc: 0.6931 - val_loss: 0.7745 - val_acc: 0.7066 - lr: 5.0000e-04\n",
      "Epoch 31/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.7440 - acc: 0.7140\n",
      "Epoch 31: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.7439 - acc: 0.7140 - val_loss: 0.7021 - val_acc: 0.7258 - lr: 5.0000e-04\n",
      "Epoch 32/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.7435 - acc: 0.7214\n",
      "Epoch 32: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3833/3833 [==============================] - 88s 23ms/step - loss: 0.7435 - acc: 0.7214 - val_loss: 0.7012 - val_acc: 0.7313 - lr: 5.0000e-04\n",
      "Epoch 33/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.7057 - acc: 0.7320\n",
      "Epoch 33: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 94s 24ms/step - loss: 0.7057 - acc: 0.7320 - val_loss: 0.6992 - val_acc: 0.7290 - lr: 2.5000e-04\n",
      "Epoch 34/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.7338\n",
      "Epoch 34: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 99s 26ms/step - loss: 0.6923 - acc: 0.7338 - val_loss: 0.8850 - val_acc: 0.7104 - lr: 2.5000e-04\n",
      "Epoch 35/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.7223 - acc: 0.7227\n",
      "Epoch 35: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 104s 27ms/step - loss: 0.7222 - acc: 0.7228 - val_loss: 0.7274 - val_acc: 0.7233 - lr: 2.5000e-04\n",
      "Epoch 36/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.7060 - acc: 0.7311\n",
      "Epoch 36: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 107s 28ms/step - loss: 0.7060 - acc: 0.7311 - val_loss: 0.6757 - val_acc: 0.7415 - lr: 2.5000e-04\n",
      "Epoch 37/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.7379\n",
      "Epoch 37: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 109s 29ms/step - loss: 0.6836 - acc: 0.7379 - val_loss: 0.6760 - val_acc: 0.7296 - lr: 2.5000e-04\n",
      "Epoch 38/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6721 - acc: 0.7383\n",
      "Epoch 38: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 115s 30ms/step - loss: 0.6721 - acc: 0.7383 - val_loss: 0.6516 - val_acc: 0.7446 - lr: 2.5000e-04\n",
      "Epoch 39/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6763 - acc: 0.7396\n",
      "Epoch 39: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 111s 29ms/step - loss: 0.6762 - acc: 0.7396 - val_loss: 0.6672 - val_acc: 0.7445 - lr: 2.5000e-04\n",
      "Epoch 40/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7436\n",
      "Epoch 40: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 103s 27ms/step - loss: 0.6727 - acc: 0.7435 - val_loss: 0.6627 - val_acc: 0.7472 - lr: 2.5000e-04\n",
      "Epoch 41/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.7521\n",
      "Epoch 41: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 106s 28ms/step - loss: 0.6513 - acc: 0.7521 - val_loss: 0.6362 - val_acc: 0.7554 - lr: 2.5000e-04\n",
      "Epoch 42/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.7464\n",
      "Epoch 42: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "3833/3833 [==============================] - 108s 28ms/step - loss: 0.6645 - acc: 0.7464 - val_loss: 0.6541 - val_acc: 0.7488 - lr: 2.5000e-04\n",
      "Epoch 43/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6575 - acc: 0.7480\n",
      "Epoch 43: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 108s 28ms/step - loss: 0.6574 - acc: 0.7481 - val_loss: 0.6527 - val_acc: 0.7501 - lr: 1.2500e-04\n",
      "Epoch 44/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6506 - acc: 0.7523\n",
      "Epoch 44: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 105s 27ms/step - loss: 0.6506 - acc: 0.7523 - val_loss: 0.6769 - val_acc: 0.7463 - lr: 1.2500e-04\n",
      "Epoch 45/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.7268 - acc: 0.7221\n",
      "Epoch 45: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 108s 28ms/step - loss: 0.7269 - acc: 0.7221 - val_loss: 0.7060 - val_acc: 0.7318 - lr: 1.2500e-04\n",
      "Epoch 46/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6851 - acc: 0.7407\n",
      "Epoch 46: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 109s 28ms/step - loss: 0.6851 - acc: 0.7407 - val_loss: 0.6591 - val_acc: 0.7481 - lr: 1.2500e-04\n",
      "Epoch 47/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6609 - acc: 0.7473\n",
      "Epoch 47: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 107s 28ms/step - loss: 0.6609 - acc: 0.7473 - val_loss: 0.6534 - val_acc: 0.7515 - lr: 1.2500e-04\n",
      "Epoch 48/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6699 - acc: 0.7477\n",
      "Epoch 48: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6698 - acc: 0.7477 - val_loss: 0.6780 - val_acc: 0.7536 - lr: 1.2500e-04\n",
      "Epoch 49/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.7496\n",
      "Epoch 49: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6586 - acc: 0.7495 - val_loss: 0.6445 - val_acc: 0.7547 - lr: 1.2500e-04\n",
      "Epoch 50/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6511 - acc: 0.7508\n",
      "Epoch 50: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6511 - acc: 0.7508 - val_loss: 0.6431 - val_acc: 0.7547 - lr: 1.2500e-04\n",
      "Epoch 51/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.7503\n",
      "Epoch 51: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 96s 25ms/step - loss: 0.6542 - acc: 0.7502 - val_loss: 0.6645 - val_acc: 0.7451 - lr: 1.2500e-04\n",
      "Epoch 52/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.7449\n",
      "Epoch 52: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "3833/3833 [==============================] - 95s 25ms/step - loss: 0.6640 - acc: 0.7449 - val_loss: 0.6536 - val_acc: 0.7484 - lr: 1.2500e-04\n",
      "Epoch 53/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6652 - acc: 0.7463\n",
      "Epoch 53: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 95s 25ms/step - loss: 0.6652 - acc: 0.7463 - val_loss: 0.6528 - val_acc: 0.7531 - lr: 6.2500e-05\n",
      "Epoch 54/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6535 - acc: 0.7490\n",
      "Epoch 54: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 96s 25ms/step - loss: 0.6535 - acc: 0.7489 - val_loss: 0.6418 - val_acc: 0.7531 - lr: 6.2500e-05\n",
      "Epoch 55/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.7508\n",
      "Epoch 55: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 94s 25ms/step - loss: 0.6507 - acc: 0.7508 - val_loss: 0.6482 - val_acc: 0.7506 - lr: 6.2500e-05\n",
      "Epoch 56/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.7529\n",
      "Epoch 56: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 95s 25ms/step - loss: 0.6383 - acc: 0.7529 - val_loss: 0.6266 - val_acc: 0.7570 - lr: 6.2500e-05\n",
      "Epoch 57/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6418 - acc: 0.7541\n",
      "Epoch 57: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 97s 25ms/step - loss: 0.6418 - acc: 0.7541 - val_loss: 0.6716 - val_acc: 0.7450 - lr: 6.2500e-05\n",
      "Epoch 58/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6715 - acc: 0.7470\n",
      "Epoch 58: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 97s 25ms/step - loss: 0.6715 - acc: 0.7471 - val_loss: 0.6569 - val_acc: 0.7470 - lr: 6.2500e-05\n",
      "Epoch 59/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6583 - acc: 0.7486\n",
      "Epoch 59: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 89s 23ms/step - loss: 0.6583 - acc: 0.7486 - val_loss: 0.6466 - val_acc: 0.7517 - lr: 6.2500e-05\n",
      "Epoch 60/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6847 - acc: 0.7519\n",
      "Epoch 60: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 90s 23ms/step - loss: 0.6847 - acc: 0.7519 - val_loss: 0.8521 - val_acc: 0.7512 - lr: 6.2500e-05\n",
      "Epoch 61/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6426 - acc: 0.7517\n",
      "Epoch 61: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6425 - acc: 0.7517 - val_loss: 0.6429 - val_acc: 0.7462 - lr: 6.2500e-05\n",
      "Epoch 62/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.7521\n",
      "Epoch 62: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "3833/3833 [==============================] - 107s 28ms/step - loss: 0.6435 - acc: 0.7521 - val_loss: 0.6351 - val_acc: 0.7548 - lr: 6.2500e-05\n",
      "Epoch 63/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6337 - acc: 0.7544\n",
      "Epoch 63: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 101s 26ms/step - loss: 0.6337 - acc: 0.7544 - val_loss: 0.6295 - val_acc: 0.7543 - lr: 3.1250e-05\n",
      "Epoch 64/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6342 - acc: 0.7560\n",
      "Epoch 64: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 91s 24ms/step - loss: 0.6342 - acc: 0.7559 - val_loss: 0.6250 - val_acc: 0.7544 - lr: 3.1250e-05\n",
      "Epoch 65/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6292 - acc: 0.7570\n",
      "Epoch 65: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 91s 24ms/step - loss: 0.6292 - acc: 0.7570 - val_loss: 0.6353 - val_acc: 0.7545 - lr: 3.1250e-05\n",
      "Epoch 66/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.7559\n",
      "Epoch 66: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 92s 24ms/step - loss: 0.6348 - acc: 0.7559 - val_loss: 0.6401 - val_acc: 0.7559 - lr: 3.1250e-05\n",
      "Epoch 67/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6342 - acc: 0.7553\n",
      "Epoch 67: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 91s 24ms/step - loss: 0.6342 - acc: 0.7553 - val_loss: 0.6566 - val_acc: 0.7587 - lr: 3.1250e-05\n",
      "Epoch 68/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6316 - acc: 0.7575\n",
      "Epoch 68: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6316 - acc: 0.7575 - val_loss: 0.6308 - val_acc: 0.7616 - lr: 3.1250e-05\n",
      "Epoch 69/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.7593\n",
      "Epoch 69: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 92s 24ms/step - loss: 0.6294 - acc: 0.7593 - val_loss: 0.6272 - val_acc: 0.7605 - lr: 3.1250e-05\n",
      "Epoch 70/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6269 - acc: 0.7593\n",
      "Epoch 70: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 92s 24ms/step - loss: 0.6269 - acc: 0.7593 - val_loss: 0.6263 - val_acc: 0.7593 - lr: 3.1250e-05\n",
      "Epoch 71/120\n",
      "3833/3833 [==============================] - ETA: 0s - loss: 0.6254 - acc: 0.7591\n",
      "Epoch 71: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6254 - acc: 0.7591 - val_loss: 0.6367 - val_acc: 0.7561 - lr: 3.1250e-05\n",
      "Epoch 72/120\n",
      "3831/3833 [============================>.] - ETA: 0s - loss: 0.6306 - acc: 0.7565\n",
      "Epoch 72: val_acc did not improve from 0.84120\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "3833/3833 [==============================] - 93s 24ms/step - loss: 0.6306 - acc: 0.7565 - val_loss: 0.6262 - val_acc: 0.7586 - lr: 3.1250e-05\n",
      "Epoch 73/120\n",
      "3832/3833 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.7592\n",
      "Epoch 73: val_acc did not improve from 0.84120\n",
      "3833/3833 [==============================] - 96s 25ms/step - loss: 0.6244 - acc: 0.7592 - val_loss: 0.6261 - val_acc: 0.7587 - lr: 1.5625e-05\n",
      "Epoch 74/120\n",
      "1270/3833 [========>.....................] - ETA: 1:02 - loss: 0.6235 - acc: 0.7600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17792\\3828824450.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m     callbacks=[\n\u001B[0;32m      9\u001B[0m         \u001B[0mModelCheckpoint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'models/model_r8.h5'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmonitor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'val_acc'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_best_only\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'auto'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m         \u001B[0mReduceLROnPlateau\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmonitor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'val_acc'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfactor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'auto'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m     ]\n\u001B[0;32m     12\u001B[0m )\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1648\u001B[0m                         ):\n\u001B[0;32m   1649\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1650\u001B[1;33m                             \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1651\u001B[0m                             \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1652\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 150\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    878\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    879\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 880\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    881\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    882\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    910\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    911\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 912\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_no_variable_creation_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    913\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_variable_creation_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    133\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m    134\u001B[0m     return concrete_function._call_flat(\n\u001B[1;32m--> 135\u001B[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[0;32m    136\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1744\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1745\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1746\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1747\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1748\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    381\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    382\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 383\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    384\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    385\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mD:\\Python 3.7.8\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 53\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     54\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=120,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model_r8.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T20:29:28.539339300Z",
     "start_time": "2023-11-13T20:29:28.539339300Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T20:29:28.544338900Z",
     "start_time": "2023-11-13T20:29:28.541339200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model_r8.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T20:29:28.544338900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
